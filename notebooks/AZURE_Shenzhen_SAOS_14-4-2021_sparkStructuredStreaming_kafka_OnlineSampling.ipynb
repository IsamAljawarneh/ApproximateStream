{"nbformat_minor": 4, "cells": [{"source": "# approximate spatial query processing on Microsoft Azure with Apache Spark and Kafka", "cell_type": "markdown", "metadata": {}}, {"source": "# configurations\n***\n\n***", "cell_type": "markdown", "metadata": {}}, {"source": "**you need to import the following libraries**\n```\n- org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.0\n- com.esri.geometry:esri-geometry-api:1.2.1\n- commons-io:commons-io:2.6\n- org.apache.spark:spark-streaming_2.11:2.2.0\n```\n* you should do this in the `\"spark.jars.packages\"` section of the  `%%configure -f` spark magic cell below\n* for the `magellan` spatial library, you need to upload the fat .jar file to the `storage account` `container` of your Azure Spark cluster [instructions here](https://github.com/IsamAljawarneh/ApproximateStream/blob/master/instructions/run_on_Azure.md)\n    - suppose you have uploaded that to a folder titled `jars`, OR replace the `FOLDER_NAME` with the folder name where you have placed magellan`\n    - then use the directive `\"spark.jars\"` section of the  `%%configure -f` spark magic cell below to import this library\n    - replace `CONTAINER_NAME` with the container name in your Spark storage account where you hosted the `magellan` spatial library. ALSO, replace `STORAGE_ACCOUNT_NAME` with the name of your Spark `storage account`", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "%%configure -f\n{\n    \"conf\": {\n        \"spark.jars.packages\": \"org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.0,com.esri.geometry:esri-geometry-api:1.2.1,commons-io:commons-io:2.6,org.apache.spark:spark-streaming_2.11:2.2.0\",\n        \"spark.jars\":\"wasbs://CONTAINER_NAME@STORAGE_ACCOUNT_NAME.blob.core.windows.net/FOLDER_NAME/magellan-1.0.5-s_2.11.jar\",\n        \"spark.jars.excludes\": \"org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11\",\n        \"spark.dynamicAllocation.enabled\": false\n    }\n}\n", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "/**\n * @Description: Approximate Spatial Query Processing on Azure with Spark and Kafka\n * @author: Isam Al Jawarneh\n * @date: 02/04/2021\n */", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 755.83203125, "end_time": 1625096330264.916}}, "editable": true, "collapsed": false, "deletable": true}}, {"source": "# parameters", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "//parameters\n\nval sampling_fraction = 0.2\nval precision = 25\n", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 1277.263916015625, "end_time": 1625096361633.301}}, "editable": true, "collapsed": false, "deletable": true}}, {"source": "# import", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "import util.control.Breaks._\nimport org.apache.spark.sql.streaming.StreamingQueryListener\n//import org.apache.spark.util.random.XORShiftRandom\nimport org.apache.spark.sql.functions.col\nimport org.apache.spark.sql.types._\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkConf\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.SQLImplicits\nimport org.apache.spark.sql.functions.from_json\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.DataFrame\nimport org.apache.spark.sql.Dataset\nimport org.apache.spark.sql.ForeachWriter\nimport magellan._\nimport magellan.index.ZOrderCurve\nimport magellan.{Point, Polygon}\n\nimport org.apache.spark.sql.magellan.dsl.expressions._\nimport org.apache.spark.sql.Row\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.streaming.OutputMode\nimport org.apache.spark.sql.types.{\n  DoubleType,\n  StringType,\n  StructField,\n  StructType\n}\nimport org.apache.spark.sql.streaming._\nimport org.apache.spark.sql.streaming.Trigger\nimport org.apache.spark.sql.execution.streaming.MemoryStream\nimport org.apache.spark.sql.functions.{collect_list, collect_set}\nimport org.apache.spark.sql.SQLContext\nimport org.apache.log4j.{Level, Logger}\nimport scala.collection.mutable\nimport scala.concurrent.duration.Duration\nimport java.io.{BufferedWriter, FileWriter}\nimport org.apache.commons.io.FileUtils\nimport java.io.File\nimport scala.collection.mutable.ListBuffer\nimport java.time.Instant\n//import org.apache.spark.util.CollectionAccumulator\nimport org.apache.spark.sql.DataFrame\nimport org.apache.kafka.clients.consumer.ConsumerRecord\nimport org.apache.kafka.common.serialization.StringDeserializer", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 7311.13720703125, "end_time": 1625096369826.443}}, "editable": true, "collapsed": false, "deletable": true}}, {"source": "# creating a Kafka topic\n* in the following cell, we create a kafka topic\n    - replace `TOPIC_NAME` with the name of topic of your choice, i used `spatial1` here\n    - replace `HOST_INFO` with the host information you gathered using jq, that is a list that should end with `2181` for each element [instructions here](https://github.com/IsamAljawarneh/ApproximateStream/blob/master/instructions/run_on_Azure.md)\n    - you gather the information with the following command sequence\n    ```\nexport password='KAFKA_CLUSTER_PASS'\nexport CLUSTERNAME=KAFKA_CLUSTER_NAME\n```\n`\n    curl -sS -u admin:$password -G \"https://skafka.azurehdinsight.net/api/v1/clusters/skafka/services/ZOOKEEPER/components/ZOOKEEPER_SERVER\" | jq -r '[\"\\(.host_components[].HostRoles.host_name):2181\"] | join(\",\")' | cut -d',' -f1,2\n`\n    - replace `KAFKA_CLUSTER_PASS` with your kafka clusetr passs, and `KAFKA_CLUSTER_NAME` with you kafka cluster name\n- N.B **you create the topic once and disable the cell**", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "%%bash\n\n#create topic 'spatial1' with 16 partitions \nexport KafkaZookeepers=\"HOST_INFO\"\n\n/usr/hdp/current/kafka-broker/bin/kafka-topics.sh --create --replication-factor 1 --partitions 16 --topic TOPIC_NAME --zookeeper $KafkaZookeepers\n", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "%%bash\n#list topics to check\nexport KafkaZookeepers=\"HOST_INFO\"\n/usr/hdp/current/kafka-broker/bin/kafka-topics.sh --list --zookeeper $KafkaZookeepers", "outputs": [], "metadata": {"collapsed": true}}, {"source": "# Kafka brokers\n* get kafka brokers (ending with 9092) [instructions here](https://github.com/IsamAljawarneh/ApproximateStream/blob/master/instructions/run_on_Azure.md)", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "val kafkaBrokers=\"KAFKA_BROKERS\"\n", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "\nval stream = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", kafkaBrokers).option(\"startingOffsets\", \"earliest\").option(\"subscribe\", \"spatial1\").load()//.option(\"maxOffsetsPerTrigger\",2).option(\"startingOffsets\", \"earliest\")", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 3497.14892578125, "end_time": 1625096378465.452}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "val schemaNYCshort = StructType(Array(\n    StructField(\"id\", StringType, false),\n    StructField(\"lat\", DoubleType, false),\n    StructField(\"lon\", DoubleType, false),\n    StructField(\"time\", StringType, false),\n    StructField(\"speed\", DoubleType, false)))", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 1263.428955078125, "end_time": 1625096379739.482}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "val geohashUDF = udf{(curve: Seq[ZOrderCurve]) => curve.map(_.toBase32())}", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 1277.242919921875, "end_time": 1625096381028.713}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "val transformationStream1 = stream.selectExpr(\"CAST(value AS STRING)\").as[(String)].select(from_json($\"value\", schemaNYCshort).as(\"data\")).select(\"data.*\")\n\nval ridesGeohashed = transformationStream1.withColumn(\"point\", point($\"lat\",$\"lon\")).withColumn(\"index\", $\"point\" index  precision).withColumn(\"geohashArray\", geohashUDF($\"index.curve\")).select( $\"point\",$\"geohashArray\",$\"speed\")\nval dataStream1 = ridesGeohashed.explode(\"geohashArray\", \"geohash\") { a: mutable.WrappedArray[String] => a }\nval transformationStream = dataStream1.select(\"point\",\"speed\", \"geohash\")//.groupBy(\"geohash\").count().orderBy($\"count\".desc) //", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 3282.785888671875, "end_time": 1625096384323.707}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "\n def getSamplingRate1(map: Map[String, Double], defaultValue: Double) = udf{\n  (geohash: String, rnd: Double) =>\n      rnd < map.getOrElse(geohash.asInstanceOf[String], 0.0)\n}\n\ndef spatialSampleBy(neigh_geohashed_df:DataFrame, points_geohashed_df:DataFrame, samplingRatio: Double): DataFrame = {\n    val geoSeq: Seq[String] = neigh_geohashed_df.select(\"geohash\").distinct.rdd.map(r => r(0).asInstanceOf[String]).collect()\n    val map = Map(geoSeq map { a => a -> samplingRatio }: _*)\n\n        val tossAcoin = rand(7L)\n    val getSamplingRate = udf { (geohash: Any, rnd: Double) =>\n      rnd < map.getOrElse(geohash.asInstanceOf[String], 0.0)\n    }\nval samplepointDF =  points_geohashed_df.filter(getSamplingRate1(map, 0.0)($\"geohash\", tossAcoin))\n    return samplepointDF}\n", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 1275.546875, "end_time": 1625096385610.662}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "def geohashedNeighborhoods(geohashPrecision: Int, filePath: String): DataFrame = \n\n{\n\nimport spark.implicits._\n/*preparing the neighborhoods table (static table) .... getting geohashes covering for every neighborhood and \nexploding it, so that each neighborhood has many geohashes*/\n\n// this will be executed only one time - batch mode \nval rawNeighborhoods = spark.sqlContext.read.format(\"magellan\").option(\"type\", \"geojson\").load(filePath).select($\"polygon\", $\"metadata\"(\"NAME\").as(\"neighborhood\"))//.cache()\n\nval neighborhoods = rawNeighborhoods.withColumn(\"index\", $\"polygon\" index geohashPrecision).select($\"polygon\", $\"index\", \n      $\"neighborhood\")//.cache()\n    //print(neighborhoods.count())\n\nval zorderIndexedNeighborhoods = neighborhoods.withColumn(\"index\", explode($\"index\")).select(\"polygon\", \"index.curve\", \"index.relation\",\"neighborhood\")\nval geohashedNeighborhoods= neighborhoods.withColumn(\"geohashArray\", geohashUDF($\"index.curve\"))\n\nval explodedgeohashedNeighborhoods = geohashedNeighborhoods.explode(\"geohashArray\", \"geohash\") { a: mutable.WrappedArray[String] => a }\n\n//unit testing: explodedgeohashedNeighborhoods.show(10)\n\n\nexplodedgeohashedNeighborhoods\n\n}", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 765.655029296875, "end_time": 1625096386388.557}}, "editable": true, "collapsed": false, "deletable": true}}, {"source": "# retreiving the regions file\n* replace `CONTAINER_NAME` with the container name in your Spark storage account where you hosted the `magellan` spatial library. ALSO, replace `STORAGE_ACCOUNT_NAME` with the name of your Spark `storage account` ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "\nval geohashedNeigboors = geohashedNeighborhoods(precision,\"wasb://CONTAINER_NAME@STORAGE_ACCOUNT_NAME.blob.core.windows.net/datasets/shenzhen_converted.geojson\")", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "//geohashedNeigboors.dropDuplicates(\"geohash\").show(2)\ngeohashedNeigboors.count()", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 5289.485107421875, "end_time": 1625096514934.554}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "//val population = spark.sql(\"select * from queryTable\")", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 741.886962890625, "end_time": 1618433168716.992}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "\nval samplepointDF_SSS = spatialSampleBy(geohashedNeigboors,transformationStream,sampling_fraction)\n", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 5297.650146484375, "end_time": 1625096530002.061}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "//run all above", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 744.473876953125, "end_time": 1618421687200.873}}, "editable": true, "collapsed": false, "deletable": true}}, {"source": "## ramp-up: generate some records for kafka before running this cell\n- [instructions here](https://github.com/IsamAljawarneh/ApproximateStream/blob/master/instructions/run_on_Azure.md) ", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "\nval samplingStatisticsDF  = samplepointDF_SSS.groupBy($\"geohash\").agg(\n    avg($\"speed\").as(\"per_strat_mean\"), variance($\"speed\").as(\"per_strat_var\")//.cast(\"double\"),\n    ,sum($\"speed\").as(\"per_strat_sum\"),\n    count($\"speed\").cast(\"double\").as(\"per_strat_count\")).withColumn(\"NhYbarh\",col(\"per_strat_mean\")* col(\"per_strat_count\")).withColumn(\"quantity\",when($\"per_strat_var\".isNaN, lit(0)).otherwise((lit(1) - (col(\"per_strat_count\")/(col(\"per_strat_count\")/lit( sampling_fraction)))) * (col(\"per_strat_count\")/lit( sampling_fraction)) * (col(\"per_strat_count\")/lit( sampling_fraction)) * (col(\"per_strat_var\")/col(\"per_strat_count\")))).withColumn(\"origin_strat_count\",col(\"per_strat_count\")/sampling_fraction)\n\n ", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 758.501953125, "end_time": 1625096672872.713}}, "editable": true, "collapsed": false, "deletable": true}}, {"source": "## before running this cell run the kafka producer in the kafka cluster head node", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "//run the kafka-java-producer before running this cell\n/*\nthereafter, we output data to a local in-memory sink\nto be able to perform queries locally over already-aggregated stream data.\nso, this way we are writing only sumamries in-memory, which is more effecient\n*/\nval points_new = samplingStatisticsDF.writeStream.queryName(\"queryTable\").format(\"memory\").outputMode(\"complete\").start()//outputMode(\"append\")\n", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 2275.090087890625, "end_time": 1625096681991.094}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "//check wether the stream is active\npoints_new.isActive", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 770.385986328125, "end_time": 1625096683884.999}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "//points_new.stop", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 741.818115234375, "end_time": 1618433212389.716}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "\nval monitorProgress = new scala.collection.mutable.ListBuffer[Double]()\nval monitorTuplesCount = new scala.collection.mutable.ListBuffer[Double]()\nval monitorError = new scala.collection.mutable.ListBuffer[Double]()\n\n\nvar values:scala.collection.mutable.Map[Int,Double] = scala.collection.mutable.Map()\nvar temp = 0.0\nvar population_total = 0\nvar sample_sum:Double = 0\nvar y_bar = 0.0\nval batch_interval = 10\n\nnew Thread(new Runnable() {\n    override def run(): Unit = {\n\nprint(points_new.isActive)\n\n      /*while (!points.isActive) {\n          Thread.sleep(100)\n        }*/\n      while (points_new.isActive ) {//start while\n          \n          \nval population = spark.sql(\"select * from queryTable\")\npopulation.createOrReplaceTempView(\"updates\")\n\nval tau_hat_str  = spark.sql(\"select sum(NhYbarh) from updates\").head().getDouble(0)\nval popTotal_from_sampling  = spark.sql(\"select sum(per_strat_count) from updates\").head().getDouble(0)\n\n val y_bar = tau_hat_str/popTotal_from_sampling\nval N = popTotal_from_sampling/sampling_fraction\nprint(y_bar)\n          \nval estimated_varianceS_estimated_total  = spark.sql(\"select sum(quantity) from updates\").head().getDouble(0)\nval popTotal_original: Double = spark.sql(\"select sum(per_strat_count) from updates\").head().getDouble(0)/sampling_fraction\nval estimated_varianceS_estimated_Mean:Double = estimated_varianceS_estimated_total/(popTotal_original*popTotal_original)\nval SE_SSS:Double = scala.math.sqrt(estimated_varianceS_estimated_Mean)\n\n          \n          monitorProgress.append(y_bar)\n          monitorTuplesCount.append(N)\n         monitorError.append(SE_SSS)\n          \n      \n          \n          if(N>=1155000){\n             points_new.stop \n          }\n          \n          Thread.sleep(10000)\n      }//end while\n    }\n  }).start()\n          \n", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 3304.4638671875, "end_time": 1625096887220.208}}, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "\nprintln(\"tuples\")\nmonitorTuplesCount.distinct.foreach(println)\n", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 761.93408203125, "end_time": 1625096891436.843}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "println(\"avg\")\nmonitorProgress.distinct.foreach(println)\n", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 752.4970703125, "end_time": 1625096896530.096}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "println(\"Standard Error\")\nmonitorError.distinct.foreach(println)\n", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 757.60302734375, "end_time": 1625096899182.893}}, "editable": true, "collapsed": false, "deletable": true}}, {"source": "## : 'To purge the Kafka topic, \nyou need to change the retention time of that topic. The default \nretention time is 168 hours, i.e. 7 days. So, you have to change the retention time to 1 second, \nafter which the messages from the topic will be deleted. Then, you can go ahead and change the retention \ntime of the topic back to 168 hours = 604800000 ms.'\n\n**after you change the retention to 1000 ms, it takes some time to empty the topic, so, if you describe the topic you may still see the partitions not empty for some time**", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "%%bash\n#list topics to check\nexport KafkaZookeepers=\"HOST_INFO\"\n/usr/hdp/current/kafka-broker/bin/kafka-topics.sh --list --zookeeper $KafkaZookeepers\n\n/usr/hdp/current/kafka-broker/bin/kafka-topics.sh  --zookeeper $KafkaZookeepers --alter --topic spatial1 --config retention.ms=604800000\n", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true, "editable": true, "deletable": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Spark", "name": "sparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-scala", "pygments_lexer": "scala", "name": "scala", "codemirror_mode": "text/x-scala"}}}